[
  {
    "title": "Fast Feature Field ($\\text{F}^3$): A Predictive Representation of Events",
    "summary": "This paper develops a mathematical argument and algorithms for building\nrepresentations of data from event-based cameras, that we call Fast Feature\nField ($\\text{F}^3$). We learn this representation by predicting future events\nfrom past events and show that it preserves scene structure and motion\ninformation. $\\text{F}^3$ exploits the sparsity of event data and is robust to\nnoise and variations in event rates. It can be computed efficiently using ideas\nfrom multi-resolution hash encoding and deep sets - achieving 120 Hz at HD and\n440 Hz at VGA resolutions. $\\text{F}^3$ represents events within a contiguous\nspatiotemporal volume as a multi-channel image, enabling a range of downstream\ntasks. We obtain state-of-the-art performance on optical flow estimation,\nsemantic segmentation, and monocular metric depth estimation, on data from\nthree robotic platforms (a car, a quadruped robot and a flying platform),\nacross different lighting conditions (daytime, nighttime), environments\n(indoors, outdoors, urban, as well as off-road) and dynamic vision sensors\n(resolutions and event rates). Our implementations can predict these tasks at\n25-75 Hz at HD resolution.",
    "published": "2025-09-29T17:52:31Z",
    "arxiv_url": "http://arxiv.org/abs/2509.25146v1",
    "arxiv_id": "2509.25146v1",
    "llm_summary": "This research paper proposes a new method for representing data from event-based cameras, called Fast Feature Field ($\\text{F}^3$), which preserves scene structure and motion information. The method is robust to noise and variations in event rates and can be computed efficiently using multi-resolution hash encoding and deep sets. The proposed method achieves state-of-the-art performance on various downstream tasks, including optical flow estimation, semantic segmentation, and monocular metric depth estimation, across different lighting conditions and environments."
  },
  {
    "title": "Paired by the Teacher: Turning Unpaired Data into High-Fidelity Pairs\n  for Low-Resource Text Generation",
    "summary": "We present Paired by the Teacher (PbT), a two-stage teacher-student pipeline\nthat synthesizes accurate input-output pairs without human labels or parallel\ndata. In many low-resource natural language generation (NLG) scenarios,\npractitioners may have only raw outputs, like highlights, recaps, or questions,\nor only raw inputs, such as articles, dialogues, or paragraphs, but seldom\nboth. This mismatch forces small models to learn from very few examples or rely\non costly, broad-scope synthetic examples produced by large LLMs. PbT addresses\nthis by asking a teacher LLM to compress each unpaired example into a concise\nintermediate representation (IR), and training a student to reconstruct inputs\nfrom IRs. This enables outputs to be paired with student-generated inputs,\nyielding high-quality synthetic data. We evaluate PbT on five\nbenchmarks-document summarization (XSum, CNNDM), dialogue summarization\n(SAMSum, DialogSum), and question generation (SQuAD)-as well as an unpaired\nsetting on SwitchBoard (paired with DialogSum summaries). An 8B student trained\nonly on PbT data outperforms models trained on 70 B teacher-generated corpora\nand other unsupervised baselines, coming within 1.2 ROUGE-L of human-annotated\npairs and closing 82% of the oracle gap at one-third the annotation cost of\ndirect synthesis. Human evaluation on SwitchBoard further confirms that only\nPbT produces concise, faithful summaries aligned with the target style,\nhighlighting its advantage of generating in-domain sources that avoid the\nmismatch, limiting direct synthesis.",
    "published": "2025-09-29T17:51:55Z",
    "arxiv_url": "http://arxiv.org/abs/2509.25144v1",
    "arxiv_id": "2509.25144v1",
    "llm_summary": "This research paper introduces Paired by the Teacher, a two-stage teacher-student pipeline that synthesizes accurate input-output pairs without human labels or parallel data. The pipeline compresses unpaired examples into concise intermediate representations and trains a student to reconstruct inputs from these representations. The results show that the pipeline outperforms other unsupervised baselines, including direct synthesis, and produces high-quality summaries aligned with the target style."
  },
  {
    "title": "The Era of Real-World Human Interaction: RL from User Conversations",
    "summary": "We posit that to achieve continual model improvement and multifaceted\nalignment, future models must learn from natural human interaction. Current\nconversational models are aligned using pre-annotated, expert-generated human\nfeedback. In this work, we introduce Reinforcement Learning from Human\nInteraction (RLHI), a paradigm that learns directly from in-the-wild user\nconversations. We develop two complementary methods: (1) RLHI with User-Guided\nRewrites, which revises unsatisfactory model outputs based on users'\nnatural-language follow-up responses, (2) RLHI with User-Based Rewards, which\nlearns via a reward model conditioned on knowledge of the user's long-term\ninteraction history (termed persona). Together, these methods link long-term\nuser personas to turn-level preferences via persona-conditioned preference\noptimization. Trained on conversations derived from WildChat, both RLHI\nvariants outperform strong baselines in personalization and\ninstruction-following, and similar feedback enhances performance on reasoning\nbenchmarks. These results suggest organic human interaction offers scalable,\neffective supervision for personalized alignment.",
    "published": "2025-09-29T17:50:31Z",
    "arxiv_url": "http://arxiv.org/abs/2509.25137v1",
    "arxiv_id": "2509.25137v1",
    "llm_summary": "Researchers propose a new approach to model improvement and human- model alignment by leveraging natural human interaction. They developed two RLHI variants that learn from user feedback and personas, which are conditioned on user interaction history. These methods have been shown to outperform strong baselines in personalization and instruction-following tasks."
  },
  {
    "title": "BALF: Budgeted Activation-Aware Low-Rank Factorization for\n  Fine-Tuning-Free Model Compression",
    "summary": "Neural network compression techniques typically require expensive fine-tuning\nor search procedures, rendering them impractical on commodity hardware.\nInspired by recent LLM compression research, we present a general\nactivation-aware factorization framework that can be applied to a broad range\nof layers. Moreover, we introduce a scalable budgeted rank allocator that\nallows flexible control over compression targets (e.g., retaining 50% of\nparameters) with no overhead. Together, these components form BALF, an\nefficient pipeline for compressing models without fine-tuning. We demonstrate\nits effectiveness across multiple scales and architectures, from ResNet-20 on\nCIFAR-10 to ResNeXt-101 and vision transformers on ImageNet, and show that it\nachieves excellent results in the fine-tuning-free regime. For instance, BALF\nreduces FLOPs on ResNeXt-101 by 45% with only a 1-percentage-point top-1\naccuracy drop.",
    "published": "2025-09-29T17:50:29Z",
    "arxiv_url": "http://arxiv.org/abs/2509.25136v1",
    "arxiv_id": "2509.25136v1",
    "llm_summary": "This research paper presents a novel activation-aware factorization framework (BALF) for neural network compression. BALF is a general framework that can be applied to various layers and architectures, and it introduces a scalable budgeted rank allocator to control compression targets. The framework has been tested on multiple scales and architectures, including ResNet-20, ResNeXt-101, and vision transformers on ImageNet, and demonstrated excellent results in the fine-tuning-free regime."
  },
  {
    "title": "Learning in an Echo Chamber: Online Learning with Replay Adversary",
    "summary": "As machine learning systems increasingly train on self-annotated data, they\nrisk reinforcing errors and becoming echo chambers of their own beliefs. We\nmodel this phenomenon by introducing a learning-theoretic framework: Online\nLearning in the Replay Setting. In round $t$, the learner outputs a hypothesis\n$\\hat{h}_t$; the adversary then reveals either the true label $f^\\ast(x_t)$ or\na replayed label $\\hat{h}_i(x_t)$ from an earlier round $i < t$. A mistake is\ncounted only when the true label is shown, yet classical algorithms such as the\nSOA or the halving algorithm are easily misled by the replayed errors.\n  We introduce the Extended Threshold dimension, $\\mathrm{ExThD}(\\mathcal{H})$,\nand prove matching upper and lower bounds that make\n$\\mathrm{ExThD}(\\mathcal{H})$ the exact measure of learnability in this model.\nA closure-based learner makes at most $\\mathrm{ExThD}(\\mathcal{H})$ mistakes\nagainst any adaptive adversary, and no algorithm can perform better. For\nstochastic adversaries, we prove a similar bound for every intersection-closed\nclass. The replay setting is provably harder than the classical mistake bound\nsetting: some classes have constant Littlestone dimension but arbitrarily large\n$\\mathrm{ExThD}(\\mathcal{H})$. Proper learning exhibits an even sharper\nseparation: a class is properly learnable under replay if and only if it is\n(almost) intersection-closed. Otherwise, every proper learner suffers\n$\\Omega(T)$ errors, whereas our improper algorithm still achieves the\n$\\mathrm{ExThD}(\\mathcal{H})$ bound. These results give the first tight\nanalysis of learning against replay adversaries, based on new results for\nclosure-type algorithms.",
    "published": "2025-09-29T17:50:24Z",
    "arxiv_url": "http://arxiv.org/abs/2509.25135v1",
    "arxiv_id": "2509.25135v1",
    "llm_summary": "This research paper introduces a new framework for analyzing the learning of machine learning systems against replay adversaries. The framework, called Online Learning in the Replay Setting, is based on a new measure of learnability called the Extended Threshold dimension. The researchers prove that this measure is the exact measure of learnability in this model, and that a closure-based learner can make at most this measure mistakes against any adaptive adversary. They also show that a class is properly learnable under replay if and only if it is intersection-closed, and that a proper learner can achieve a tight bound of this measure."
  },
  {
    "title": "Rethinking Entropy Regularization in Large Reasoning Models",
    "summary": "Reinforcement learning with verifiable rewards (RLVR) has shown great promise\nin enhancing the reasoning abilities of large reasoning models (LRMs). However,\nit suffers from a critical issue: entropy collapse and premature convergence.\nNaive entropy regularization, a common approach for encouraging exploration in\nthe traditional RL literature, fails to address this problem in the context of\nLRM. Our analysis reveals that this failure stems from the vast action space\nand long trajectories in LRMs, which easily trigger a global entropy explosion\nas the model indiscriminately explores all possible actions and states. To\naddress this, we propose SIREN (SelectIve entRopy rEgularizatioN), a method\nthat confines exploration to a meaningful subset of actions and states. SIREN\nachieves this through a two-step entropy masking mechanism, consisting of a\ntop-p mask and a peak-entropy mask. In addition, regularization is transformed\ninto a self-anchored form to stabilize training. Across five mathematical\nbenchmarks, SIREN attains superior average performance over previous\nentropy-related RLVR approaches, exemplified by a +6.6 maj@k improvement on\nAIME24/25 with Qwen2.5-Math-7B. Further analysis confirms that SIREN promotes\ngreater response diversity and maintains entropy at an appropriate level, which\nhelps to preserve the validation pass@k throughout training. This effectively\nmitigates the premature convergence problem common in RLVR for LRM.",
    "published": "2025-09-29T17:49:25Z",
    "arxiv_url": "http://arxiv.org/abs/2509.25133v1",
    "arxiv_id": "2509.25133v1",
    "llm_summary": "This study proposes SIREN, a method to address the premature convergence problem in reinforcement learning with large reasoning models. SIREN uses a two-step entropy masking mechanism to confine exploration and a self-anchored regularization form to stabilize training. The proposed method achieves superior performance over previous approaches and promotes greater response diversity and entropy at an appropriate level, effectively mitigating premature convergence."
  },
  {
    "title": "Score Distillation of Flow Matching Models",
    "summary": "Diffusion models achieve high-quality image generation but are limited by\nslow iterative sampling. Distillation methods alleviate this by enabling one-\nor few-step generation. Flow matching, originally introduced as a distinct\nframework, has since been shown to be theoretically equivalent to diffusion\nunder Gaussian assumptions, raising the question of whether distillation\ntechniques such as score distillation transfer directly. We provide a simple\nderivation -- based on Bayes' rule and conditional expectations -- that unifies\nGaussian diffusion and flow matching without relying on ODE/SDE formulations.\nBuilding on this view, we extend Score identity Distillation (SiD) to\npretrained text-to-image flow-matching models, including SANA, SD3-Medium,\nSD3.5-Medium/Large, and FLUX.1-dev, all with DiT backbones. Experiments show\nthat, with only modest flow-matching- and DiT-specific adjustments, SiD works\nout of the box across these models, in both data-free and data-aided settings,\nwithout requiring teacher finetuning or architectural changes. This provides\nthe first systematic evidence that score distillation applies broadly to\ntext-to-image flow matching models, resolving prior concerns about stability\nand soundness and unifying acceleration techniques across diffusion- and\nflow-based generators. We will make the PyTorch implementation publicly\navailable.",
    "published": "2025-09-29T17:45:48Z",
    "arxiv_url": "http://arxiv.org/abs/2509.25127v1",
    "arxiv_id": "2509.25127v1",
    "llm_summary": "Researchers have developed a method called Score Distillation (SiD) that can be used to generate images using diffusion models, which are known for their high-quality results. The method has been shown to work well with various text-to-image flow-matching models, including those pre-trained on large datasets. By extending SiD to these models, researchers have found that it can be used to generate images with high-quality results, without requiring significant adjustments or changes to the model architecture."
  },
  {
    "title": "On Spectral Learning for Odeco Tensors: Perturbation, Initialization,\n  and Algorithms",
    "summary": "We study spectral learning for orthogonally decomposable (odeco) tensors,\nemphasizing the interplay between statistical limits, optimization geometry,\nand initialization. Unlike matrices, recovery for odeco tensors does not hinge\non eigengaps, yielding improved robustness under noise. While iterative methods\nsuch as tensor power iterations can be statistically efficient, initialization\nemerges as the main computational bottleneck. We investigate perturbation\nbounds, non-convex optimization analysis, and initialization strategies,\nclarifying when efficient algorithms attain statistical limits and when\nfundamental barriers remain.",
    "published": "2025-09-29T17:45:35Z",
    "arxiv_url": "http://arxiv.org/abs/2509.25126v1",
    "arxiv_id": "2509.25126v1",
    "llm_summary": "This research paper explores the challenges of spectral learning for orthogonally decomposable tensors, particularly in the context of statistical limits, optimization geometry, and initialization. The authors investigate the interplay between these factors to identify the main computational bottlenecks in recovery algorithms, and propose strategies to overcome them. They also discuss the implications of their findings for the development of more robust and efficient algorithms for tensor recovery."
  },
  {
    "title": "Towards generalizable deep ptychography neural networks",
    "summary": "X-ray ptychography is a data-intensive imaging technique expected to become\nubiquitous at next-generation light sources delivering many-fold increases in\ncoherent flux. The need for real-time feedback under accelerated acquisition\nrates motivates surrogate reconstruction models like deep neural networks,\nwhich offer orders-of-magnitude speedup over conventional methods. However,\nexisting deep learning approaches lack robustness across diverse experimental\nconditions. We propose an unsupervised training workflow emphasizing probe\nlearning by combining experimentally-measured probes with synthetic,\nprocedurally generated objects. This probe-centric approach enables a single\nphysics-informed neural network to reconstruct unseen experiments across\nmultiple beamlines; among the first demonstrations of multi-probe\ngeneralization. We find probe learning is equally important as in-distribution\nlearning; models trained using this synthetic workflow achieve reconstruction\nfidelity comparable to those trained exclusively on experimental data, even\nwhen changing the type of synthetic training object. The proposed approach\nenables training of experiment-steering models that provide real-time feedback\nunder dynamic experimental conditions.",
    "published": "2025-09-29T17:38:13Z",
    "arxiv_url": "http://arxiv.org/abs/2509.25104v1",
    "arxiv_id": "2509.25104v1",
    "llm_summary": "Researchers propose a new unsupervised training workflow for deep neural networks that combines experimentally-measured probes with synthetic objects to improve the robustness and generalizability of deep learning models. This approach enables the training of models that can adapt to diverse experimental conditions, including changing beamline configurations. The proposed method achieves comparable reconstruction fidelity to in-distribution training, even under dynamic experimental conditions."
  },
  {
    "title": "ORPO-Distill: Mixed-Policy Preference Optimization for\n  Cross-Architecture LLM Distillation",
    "summary": "We introduce ORPO-Distill, a general-purpose method for cross-architecture\nLLM distillation that formulates the problem as a preference optimization task.\nUnlike standard CoT distillation, the approach transfers knowledge through\ndiverse reasoning traces. It employs an Odds-Ratio Preference Optimization\nobjective that contrasts teacher and student traces for more effective\nlearning, and adopts a mixed-policy strategy for utilizing student-generated\noutputs, outperforming both off- and on-policy alternatives. Experiments on\nfive datasets and multiple student models show consistent improvements over\nconventional black-box KD baselines.",
    "published": "2025-09-29T17:34:02Z",
    "arxiv_url": "http://arxiv.org/abs/2509.25100v1",
    "arxiv_id": "2509.25100v1",
    "llm_summary": "This research introduces a new method for distilling knowledge from large language models (LLMs) called ORPO-Distill. Unlike existing methods, ORPO-Distill transfers knowledge through diverse reasoning traces, and uses an odds-ratio preference optimization objective to improve learning. Experiments on five datasets and multiple student models demonstrate the effectiveness of ORPO-Distill over conventional baselines."
  },
  {
    "title": "Curriculum Imitation Learning of Distributed Multi-Robot Policies",
    "summary": "Learning control policies for multi-robot systems (MRS) remains a major\nchallenge due to long-term coordination and the difficulty of obtaining\nrealistic training data. In this work, we address both limitations within an\nimitation learning framework. First, we shift the typical role of Curriculum\nLearning in MRS, from scalability with the number of robots, to focus on\nimproving long-term coordination. We propose a curriculum strategy that\ngradually increases the length of expert trajectories during training,\nstabilizing learning and enhancing the accuracy of long-term behaviors. Second,\nwe introduce a method to approximate the egocentric perception of each robot\nusing only third-person global state demonstrations. Our approach transforms\nidealized trajectories into locally available observations by filtering\nneighbors, converting reference frames, and simulating onboard sensor\nvariability. Both contributions are integrated into a physics-informed\ntechnique to produce scalable, distributed policies from observations. We\nconduct experiments across two tasks with varying team sizes and noise levels.\nResults show that our curriculum improves long-term accuracy, while our\nperceptual estimation method yields policies that are robust to realistic\nuncertainty. Together, these strategies enable the learning of robust,\ndistributed controllers from global demonstrations, even in the absence of\nexpert actions or onboard measurements.",
    "published": "2025-09-29T17:31:48Z",
    "arxiv_url": "http://arxiv.org/abs/2509.25097v1",
    "arxiv_id": "2509.25097v1",
    "llm_summary": "Researchers propose a new approach to learning control policies for multi-robot systems (MRS) by shifting the focus from scalability to long-term coordination and introducing a method to approximate robot egocentric perception using global state demonstrations. The proposed approach combines a curriculum strategy with a physics-informed technique to produce scalable, distributed policies from observations, enabling the learning of robust, distributed controllers from global demonstrations. The results show improved long-term accuracy and robustness to uncertainty in the proposed approach."
  },
  {
    "title": "Benchmarking ECG Foundational Models: A Reality Check Across Clinical\n  Tasks",
    "summary": "The 12-lead electrocardiogram (ECG) is a long-standing diagnostic tool. Yet\nmachine learning for ECG interpretation remains fragmented, often limited to\nnarrow tasks or datasets. Foundation models promise broader adaptability, but\ntheir generalization across diverse ECG tasks is not well understood. We\nbenchmarked eight ECG foundation models on 26 clinically relevant tasks using\n12 public datasets comprising 1,650 regression and classification targets.\nModels were evaluated under fine-tuning and frozen settings, with scaling\nanalyses across dataset sizes. Results show heterogeneous performance across\ndomains: in the most widely studied domain, adult ECG interpretation, three\nfoundation models consistently outperformed strong supervised baselines. In\ncontrast, ECG-CPC, a compact structured state-space model pretrained on HEEDB,\ndominated other categories where most foundation models failed to surpass\nsupervised learning. Foundation models also displayed distinct scaling\nbehaviors with dataset size, which are critical for small-scale clinical\napplications. Overall, while foundation models show promise for adult ECG\nanalysis, substantial gaps remain in cardiac structure, outcome prediction, and\npatient characterization. Notably, ECG-CPC's strong performance despite being\norders of magnitude smaller and consuming minimal computational resources\nhighlights untapped opportunities for advancing ECG foundation models.",
    "published": "2025-09-29T17:29:48Z",
    "arxiv_url": "http://arxiv.org/abs/2509.25095v1",
    "arxiv_id": "2509.25095v1",
    "llm_summary": "This study evaluated the performance of eight ECG foundation models on 26 clinical tasks using 12 public datasets. The results show that foundation models can perform well in adult ECG interpretation, but their performance varies across different tasks and datasets. The study also highlights the potential of compact structured state-space models like ECG-CPC, which can outperform other foundation models despite being smaller and more computationally efficient."
  },
  {
    "title": "Scaling with Collapse: Efficient and Predictable Training of LLM\n  Families",
    "summary": "Effective LLM training relies on *consistency*, meaning that key quantities\n-- such as final losses and optimal hyperparameters -- scale predictably across\nmodel sizes. Qiu et al. (2025) recently showed that this consistency extends\nbeyond scalars: whole training loss curves can *collapse* onto a universal\ntrajectory after a simple normalization. What remains unclear is whether this\nphenomenon holds for LLM families trained under *practical scaling recipes*,\nwhere width, depth, learning rate, batch size, and weight decay are scaled\njointly. We show that it does: loss curves collapse across scales precisely\nwhen optimization hyperparameters are set optimally for the given data budget,\nin accordance with recent empirical scaling laws. Collapse thus emerges as a\nsignature of compute-efficient training. We demonstrate two applications at\nscale: (1) deviation-from-collapse provides a sensitive, early diagnostic of\ntraining pathologies, and (2) the predictability of collapsed curves enables\nearly stopping in large-scale hyperparameter tuning. Finally, we train a\ncompetitive LLM family, *Celerity*, using these insights, highlighting collapse\nas an effective tool for developing efficient LLMs.",
    "published": "2025-09-29T17:26:11Z",
    "arxiv_url": "http://arxiv.org/abs/2509.25087v1",
    "arxiv_id": "2509.25087v1",
    "llm_summary": "Researchers Qiu et al. (2025) found that training large language models (LLMs) on consistent data can lead to a phenomenon where the training loss curve collapses onto a universal trajectory after scaling the model's parameters. This collapse is not limited to scalar quantities but also holds for LLM families trained under practical scaling recipes, where the optimization hyperparameters are set optimally for the given data budget. The researchers demonstrated two applications of this phenomenon: early diagnostic of training pathologies and early stopping in large-scale hyperparameter tuning. They also trained a competitive LLM family, Celerity, and"
  },
  {
    "title": "Scaling Generalist Data-Analytic Agents",
    "summary": "Data-analytic agents are emerging as a key catalyst for automated scientific\ndiscovery and for the vision of Innovating AI. Current approaches, however,\nrely heavily on prompt engineering over proprietary models, while open-source\nmodels struggle to face diverse-format, large-scale data files and\nlong-horizon, multi-step reasoning that real-world analytics demands. This\npaper introduces DataMind, a scalable data synthesis and agent training recipe\ndesigned to build generalist data-analytic agents. DataMind tackles three key\nchallenges in building open-source data-analytic agents, including insufficient\ndata resources, improper training strategy, and unstable code-based multi-turn\nrollout. Concretely, DataMind applies 1) a fine-grained task taxonomy and a\nrecursive easy-to-hard task composition mechanism to increase the diversity and\ndifficulty of synthesized queries; 2) a knowledge-augmented trajectory sampling\nstrategy followed by model-based and rule-based filtering; 3) a dynamically\nadjustable training objective combining both SFT and RL losses; 4) a\nmemory-frugal and stable code-based multi-turn rollout framework. Built on\nDataMind, we curate DataMind-12K, a high-quality trajectory set spanning\ndiverse domains, task categories, and data file formats for data-analytic\ntasks. Trained on DataMind-12K, our DataMind-14B achieves state-of-the-art with\nan average score of 71.16% on multiple data analysis benchmarks, outperforming\nthe strongest proprietary baselines DeepSeek-V3.1 and GPT-5. Our DataMind-7B\nalso performs best among all open-source models with a score of 68.10%. We also\nincorporate some empirical insights gained from our exploratory trials into the\nanalysis experiments, aiming to provide actionable insights about agentic\ntraining for the community. We will release DataMind-12K and DataMind-7B,14B\nfor the community's future research.",
    "published": "2025-09-29T17:23:08Z",
    "arxiv_url": "http://arxiv.org/abs/2509.25084v1",
    "arxiv_id": "2509.25084v1",
    "llm_summary": "This research paper introduces DataMind, a scalable data synthesis and agent training recipe designed to build generalist data-analytic agents. DataMind tackles three key challenges in building open-source data-analytic agents: insufficient data resources, improper training strategy, and unstable code-based multi-turn rollout. By applying a fine-grained task taxonomy, knowledge-augmented trajectory sampling, and dynamically adjustable training objectives, DataMind achieves state-of-the-art results on multiple data analysis benchmarks, outperforming proprietary baselines. The authors also curate a high-quality trajectory set for data-analytic tasks, which they"
  },
  {
    "title": "Towards a Certificate of Trust: Task-Aware OOD Detection for Scientific\n  AI",
    "summary": "Data-driven models are increasingly adopted in critical scientific fields\nlike weather forecasting and fluid dynamics. These methods can fail on\nout-of-distribution (OOD) data, but detecting such failures in regression tasks\nis an open challenge. We propose a new OOD detection method based on estimating\njoint likelihoods using a score-based diffusion model. This approach considers\nnot just the input but also the regression model's prediction, providing a\ntask-aware reliability score. Across numerous scientific datasets, including\nPDE datasets, satellite imagery and brain tumor segmentation, we show that this\nlikelihood strongly correlates with prediction error. Our work provides a\nfoundational step towards building a verifiable 'certificate of trust', thereby\noffering a practical tool for assessing the trustworthiness of AI-based\nscientific predictions. Our code is publicly available at\nhttps://github.com/bogdanraonic3/OOD_Detection_ScientificML",
    "published": "2025-09-29T17:21:25Z",
    "arxiv_url": "http://arxiv.org/abs/2509.25080v1",
    "arxiv_id": "2509.25080v1",
    "llm_summary": "Researchers propose a new method for detecting out-of-distribution data in regression tasks, which considers the joint likelihood of the input and the regression model's prediction. This approach provides a task-aware reliability score that correlates strongly with prediction error across various scientific datasets. The proposed method can be used to build a verifiable 'certificate of trust' for AI-based scientific predictions."
  },
  {
    "title": "Optimizing Privacy-Preserving Primitives to Support LLM-Scale\n  Applications",
    "summary": "Privacy-preserving technologies have introduced a paradigm shift that allows\nfor realizable secure computing in real-world systems. The significant barrier\nto the practical adoption of these primitives is the computational and\ncommunication overhead that is incurred when applied at scale. In this paper,\nwe present an overview of our efforts to bridge the gap between this overhead\nand practicality for privacy-preserving learning systems using multi-party\ncomputation (MPC), zero-knowledge proofs (ZKPs), and fully homomorphic\nencryption (FHE). Through meticulous hardware/software/algorithm co-design, we\nshow progress towards enabling LLM-scale applications in privacy-preserving\nsettings. We demonstrate the efficacy of our solutions in several contexts,\nincluding DNN IP ownership, ethical LLM usage enforcement, and transformer\ninference.",
    "published": "2025-09-29T17:16:51Z",
    "arxiv_url": "http://arxiv.org/abs/2509.25072v1",
    "arxiv_id": "2509.25072v1",
    "llm_summary": "This paper explores the development of privacy-preserving technologies, particularly in the context of learning machine learning (LLM) systems. The authors propose a multi-party computation (MPC) and zero-knowledge proof (ZKP) framework to enable secure computations at scale, while also demonstrating the efficacy of their solutions in several real-world applications. The proposed solutions aim to bridge the gap between the computational and communication overheads associated with these technologies."
  },
  {
    "title": "Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and\n  Planning",
    "summary": "The pursuit of artificial agents that can learn to master complex\nenvironments has led to remarkable successes, yet prevailing deep reinforcement\nlearning methods often rely on immense experience, encoding their knowledge\nopaquely within neural network weights. We propose a different paradigm, one in\nwhich an agent learns to play by reasoning and planning. We introduce Cogito,\nergo ludo (CEL), a novel agent architecture that leverages a Large Language\nModel (LLM) to build an explicit, language-based understanding of its\nenvironment's mechanics and its own strategy. Starting from a tabula rasa state\nwith no prior knowledge (except action set), CEL operates on a cycle of\ninteraction and reflection. After each episode, the agent analyzes its complete\ntrajectory to perform two concurrent learning processes: Rule Induction, where\nit refines its explicit model of the environment's dynamics, and Strategy and\nPlaybook Summarization, where it distills experiences into an actionable\nstrategic playbook. We evaluate CEL on diverse grid-world tasks (i.e.,\nMinesweeper, Frozen Lake, and Sokoban), and show that the CEL agent\nsuccessfully learns to master these games by autonomously discovering their\nrules and developing effective policies from sparse rewards. Ablation studies\nconfirm that the iterative process is critical for sustained learning. Our work\ndemonstrates a path toward more general and interpretable agents that not only\nact effectively but also build a transparent and improving model of their world\nthrough explicit reasoning on raw experience.",
    "published": "2025-09-29T17:02:31Z",
    "arxiv_url": "http://arxiv.org/abs/2509.25052v1",
    "arxiv_id": "2509.25052v1",
    "llm_summary": "This research paper proposes a novel agent architecture called Cogito, Ergo Ludo (CEL), which uses a Large Language Model to build an explicit understanding of its environment and its own strategy. The agent learns through a cycle of interaction and reflection, refining its model of the environment's dynamics and distilling experiences into an actionable strategic playbook. The results show that CEL can successfully master diverse grid-world tasks, and its iterative process is critical for sustained learning."
  },
  {
    "title": "Symmetry-Aware Bayesian Optimization via Max Kernels",
    "summary": "Bayesian Optimization (BO) is a powerful framework for optimizing noisy,\nexpensive-to-evaluate black-box functions. When the objective exhibits\ninvariances under a group action, exploiting these symmetries can substantially\nimprove BO efficiency. While using maximum similarity across group orbits has\nlong been considered in other domains, the fact that the max kernel is not\npositive semidefinite (PSD) has prevented its use in BO. In this work, we\nrevisit this idea by considering a PSD projection of the max kernel. Compared\nto existing invariant (and non-invariant) kernels, we show it achieves\nsignificantly lower regret on both synthetic and real-world BO benchmarks,\nwithout increasing computational complexity.",
    "published": "2025-09-29T17:02:28Z",
    "arxiv_url": "http://arxiv.org/abs/2509.25051v1",
    "arxiv_id": "2509.25051v1",
    "llm_summary": "Researchers have long considered using the max kernel, a symmetric kernel, in Bayesian Optimization (BO) to exploit group symmetries. However, the max kernel is not positive semidefinite (PSD), which is a requirement for using it in BO. This work proposes a new approach by projecting the max kernel onto a PSD subspace, which can be used in BO without incurring the costs associated with the max kernel's non-PSD nature. The proposed method achieves significant improvements in BO efficiency on both synthetic and real-world benchmarks."
  },
  {
    "title": "Advantage Weighted Matching: Aligning RL with Pretraining in Diffusion\n  Models",
    "summary": "Reinforcement Learning (RL) has emerged as a central paradigm for advancing\nLarge Language Models (LLMs), where pre-training and RL post-training share the\nsame log-likelihood formulation. In contrast, recent RL approaches for\ndiffusion models, most notably Denoising Diffusion Policy Optimization (DDPO),\noptimize an objective different from the pretraining objectives--score/flow\nmatching loss. In this work, we establish a novel theoretical analysis: DDPO is\nan implicit form of score/flow matching with noisy targets, which increases\nvariance and slows convergence. Building on this analysis, we introduce\n\\textbf{Advantage Weighted Matching (AWM)}, a policy-gradient method for\ndiffusion. It uses the same score/flow-matching loss as pretraining to obtain a\nlower-variance objective and reweights each sample by its advantage. In effect,\nAWM raises the influence of high-reward samples and suppresses low-reward ones\nwhile keeping the modeling objective identical to pretraining. This unifies\npretraining and RL conceptually and practically, is consistent with\npolicy-gradient theory, reduces variance, and yields faster convergence. This\nsimple yet effective design yields substantial benefits: on GenEval, OCR, and\nPickScore benchmarks, AWM delivers up to a $24\\times$ speedup over Flow-GRPO\n(which builds on DDPO), when applied to Stable Diffusion 3.5 Medium and FLUX,\nwithout compromising generation quality. Code is available at\nhttps://github.com/scxue/advantage_weighted_matching.",
    "published": "2025-09-29T17:02:20Z",
    "arxiv_url": "http://arxiv.org/abs/2509.25050v1",
    "arxiv_id": "2509.25050v1",
    "llm_summary": "This research paper presents a theoretical analysis of a novel RL approach for diffusion models, which is consistent with policy-gradient theory. The approach, called Advantage Weighted Matching (AWM), uses the same score/flow-matching loss as pretraining to obtain a lower-variance objective and reweights samples by their advantage. AWM delivers substantial benefits, including up to a $24\\times$ speedup over Flow-GRPO, without compromising generation quality."
  },
  {
    "title": "Efficient Hyperparameter Tuning via Trajectory Invariance Principle",
    "summary": "As hyperparameter tuning becomes increasingly costly at scale, efficient\ntuning methods are essential. Yet principles for guiding hyperparameter tuning\nremain limited. In this work, we seek to establish such principles by\nconsidering a broad range of hyperparameters, including batch size, learning\nrate, and weight decay. We identify a phenomenon we call trajectory invariance,\nwhere pre-training loss curves, gradient noise, and gradient norm exhibit\ninvariance--closely overlapping--with respect to a quantity that combines\nlearning rate and weight decay. This phenomenon effectively reduces the\noriginal two-dimensional hyperparameter space to one dimension, yielding an\nefficient tuning rule: follow the salient direction revealed by trajectory\ninvariance. Furthermore, we refine previous scaling laws and challenge several\nexisting viewpoints. Overall, our work proposes new principles for efficient\ntuning and inspires future research on scaling laws.",
    "published": "2025-09-29T17:01:19Z",
    "arxiv_url": "http://arxiv.org/abs/2509.25049v1",
    "arxiv_id": "2509.25049v1",
    "llm_summary": "Researchers investigate the phenomenon of trajectory invariance in hyperparameter tuning, where pre-training loss curves, gradient noise, and gradient norm exhibit invariance with respect to a quantity that combines learning rate and weight decay. This phenomenon reduces the original two-dimensional hyperparameter space to one dimension, yielding an efficient tuning rule that can be used to guide hyperparameter tuning. The researchers propose new principles for efficient tuning and challenge existing viewpoints."
  },
  {
    "title": "Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic\n  Architectures",
    "summary": "Despite their capabilities, Large Language Models (LLMs) remain opaque with\nlimited understanding of their internal representations. Current\ninterpretability methods, such as direct logit attribution (DLA) and sparse\nautoencoders (SAEs), provide restricted insight due to limitations such as the\nmodel's output vocabulary or unclear feature names. This work introduces\nHyperdimensional Probe, a novel paradigm for decoding information from the LLM\nvector space. It combines ideas from symbolic representations and neural\nprobing to project the model's residual stream into interpretable concepts via\nVector Symbolic Architectures (VSAs). This probe combines the strengths of SAEs\nand conventional probes while overcoming their key limitations. We validate our\ndecoding paradigm with controlled input-completion tasks, probing the model's\nfinal state before next-token prediction on inputs spanning syntactic pattern\nrecognition, key-value associations, and abstract inference. We further assess\nit in a question-answering setting, examining the state of the model both\nbefore and after text generation. Our experiments show that our probe reliably\nextracts meaningful concepts across varied LLMs, embedding sizes, and input\ndomains, also helping identify LLM failures. Our work advances information\ndecoding in LLM vector space, enabling extracting more informative,\ninterpretable, and structured features from neural representations.",
    "published": "2025-09-29T16:59:07Z",
    "arxiv_url": "http://arxiv.org/abs/2509.25045v1",
    "arxiv_id": "2509.25045v1",
    "llm_summary": "This research introduces a novel paradigm for decoding Large Language Models (LLMs) from their vector space, leveraging Symbolic Representations and Vector Symbolic Architectures (VSAs) to project the model's internal representations. The proposed probe, Hyperdimensional Probe, combines the strengths of Sparse Autoencoders (SAEs) and conventional probes to extract meaningful concepts from LLMs. The results show that the probe can reliably extract concepts across various LLMs, input domains, and embedding sizes, with potential applications in information decoding and LLM evaluation."
  },
  {
    "title": "A multiscale analysis of mean-field transformers in the moderate\n  interaction regime",
    "summary": "In this paper, we study the evolution of tokens through the depth of\nencoder-only transformer models at inference time by modeling them as a system\nof particles interacting in a mean-field way and studying the corresponding\ndynamics. More specifically, we consider this problem in the moderate\ninteraction regime, where the number $N$ of tokens is large and the inverse\ntemperature parameter $\\beta$ of the model scales together with $N$. In this\nregime, the dynamics of the system displays a multiscale behavior: a fast\nphase, where the token empirical measure collapses on a low-dimensional space,\nan intermediate phase, where the measure further collapses into clusters, and a\nslow one, where such clusters sequentially merge into a single one. We provide\na rigorous characterization of the limiting dynamics in each of these phases\nand prove convergence in the above mentioned limit, exemplifying our results\nwith some simulations.",
    "published": "2025-09-29T16:57:04Z",
    "arxiv_url": "http://arxiv.org/abs/2509.25040v1",
    "arxiv_id": "2509.25040v1",
    "llm_summary": "This paper studies the behavior of tokens in a system of particles interacting in a mean-field way. The authors model the tokens as a system of particles and study the dynamics of the system at different levels of interaction. They find that the system exhibits a multiscale behavior, with different phases of collapse and merging of clusters. The authors provide a characterization of the limiting dynamics in each phase and show convergence to a single cluster in the limit."
  },
  {
    "title": "Ultra-Fast Language Generation via Discrete Diffusion Divergence\n  Instruct",
    "summary": "Fast generation of language texts is the holy grail that people pursue in the\nAI era. In this work, we introduced Discrete Diffusion Divergence Instruct\n(DiDi-Instruct), a training-based method that leads to fast language generation\nmodels by initializing from a pre-trained (masked) discrete diffusion language\nmodel (dLLM). The resulting DiDi-Instruct model outperforms the dLLM\ncounterparts and the GPT-2 baseline with 64x acceleration. In the theoretical\npart of the paper, we build the foundation of DiDi-Instruct in a framework of\nintegral KL-divergence minimization, with practical training algorithms. We\nalso introduce techniques like grouped reward normalization, intermediate-state\nmatching, and the reward-guided ancestral sampler (RGAS) that significantly\nimprove the training stability, the model coverage, and the inference\nperformances. On OpenWebText, DiDi-Instruct outperforms all accelerated\nlanguage generation models as well as the GPT-2 baseline and the standard\ndLLMs, achieving sample perplexities ranging from 62.2 (8 NFEs) to 18.4 (128\nNFEs). These performance gains are accomplished with a negligible entropy loss\nof about 1% and 20x less additional training wall-clock time. We further\nvalidate the robustness and effectiveness of DiDi-Instruct through extensive\nablation studies, model scaling, and the generation of discrete protein\nsequences. In conclusion, DiDi-Instruct is an efficient yet effective\ndistillation method, enabling language generation in the blink of an eye. We\nwill release both code and models at github.com/haoyangzheng-ai/didi-instruct.",
    "published": "2025-09-29T16:55:44Z",
    "arxiv_url": "http://arxiv.org/abs/2509.25035v1",
    "arxiv_id": "2509.25035v1",
    "llm_summary": "This research paper introduces a training-based method called Discrete Diffusion Divergence Instruct (DiDi-Instruct) for fast language generation models. DiDi-Instruct outperforms existing models, including the GPT-2 baseline, and achieves high sample perplexities on OpenWebText. The method improves training stability, model coverage, and inference performances, while reducing entropy loss and training time. The results demonstrate the effectiveness of DiDi-Instruct in generating high-quality language texts."
  },
  {
    "title": "VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning",
    "summary": "Few-shot learning (FSL) aims to recognize novel concepts from only a few\nlabeled support samples. Recent studies enhance support features by\nincorporating additional semantic information or designing complex semantic\nfusion modules. However, they still suffer from hallucinating semantics that\ncontradict the visual evidence due to the lack of grounding in actual\ninstances, resulting in noisy guidance and costly corrections. To address these\nissues, we propose a novel framework, bridging Vision and Text with LLMs for\nFew-Shot Learning (VT-FSL), which constructs precise cross-modal prompts\nconditioned on Large Language Models (LLMs) and support images, seamlessly\nintegrating them through a geometry-aware alignment. It mainly consists of\nCross-modal Iterative Prompting (CIP) and Cross-modal Geometric Alignment\n(CGA). Specifically, the CIP conditions an LLM on both class names and support\nimages to generate precise class descriptions iteratively in a single\nstructured reasoning pass. These descriptions not only enrich the semantic\nunderstanding of novel classes but also enable the zero-shot synthesis of\nsemantically consistent images. The descriptions and synthetic images act\nrespectively as complementary textual and visual prompts, providing high-level\nclass semantics and low-level intra-class diversity to compensate for limited\nsupport data. Furthermore, the CGA jointly aligns the fused textual, support,\nand synthetic visual representations by minimizing the kernelized volume of the\n3-dimensional parallelotope they span. It captures global and nonlinear\nrelationships among all representations, enabling structured and consistent\nmultimodal integration. The proposed VT-FSL method establishes new\nstate-of-the-art performance across ten diverse benchmarks, including standard,\ncross-domain, and fine-grained few-shot learning scenarios. Code is available\nat https://github.com/peacelwh/VT-FSL.",
    "published": "2025-09-29T16:52:47Z",
    "arxiv_url": "http://arxiv.org/abs/2509.25033v1",
    "arxiv_id": "2509.25033v1",
    "llm_summary": "This research proposes a novel framework, VT-FSL, for few-shot learning that integrates vision and text through large language models. The framework uses cross-modal iterative prompting and cross-modal geometric alignment to generate precise class descriptions and synthetic images, which serve as complementary textual and visual prompts. VT-FSL achieves state-of-the-art performance across diverse few-shot learning benchmarks, demonstrating its effectiveness in bridging the gap between vision and text."
  },
  {
    "title": "Bayesian Surrogates for Risk-Aware Pre-Assessment of Aging Bridge\n  Portfolios",
    "summary": "Aging infrastructure portfolios pose a critical resource allocation\nchallenge: deciding which structures require intervention and which can safely\nremain in service. Structural assessments must balance the trade-off between\ncheaper, conservative analysis methods and accurate but costly simulations that\ndo not scale portfolio-wide. We propose Bayesian neural network (BNN)\nsurrogates for rapid structural pre-assessment of worldwide common bridge\ntypes, such as reinforced concrete frame bridges. Trained on a large-scale\ndatabase of non-linear finite element analyses generated via a parametric\npipeline and developed based on the Swiss Federal Railway's bridge portfolio,\nthe models accurately and efficiently estimate high-fidelity structural\nanalysis results by predicting code compliance factors with calibrated\nepistemic uncertainty. Our BNN surrogate enables fast, uncertainty-aware\ntriage: flagging likely critical structures and providing guidance where\nrefined analysis is pertinent. We demonstrate the framework's effectiveness in\na real-world case study of a railway underpass, showing its potential to\nsignificantly reduce costs and emissions by avoiding unnecessary analyses and\nphysical interventions across entire infrastructure portfolios.",
    "published": "2025-09-29T16:51:02Z",
    "arxiv_url": "http://arxiv.org/abs/2509.25031v1",
    "arxiv_id": "2509.25031v1",
    "llm_summary": "Researchers propose using Bayesian neural network (BNN) surrogates to rapidly assess the structural integrity of bridge types, such as reinforced concrete frame bridges, in global infrastructure portfolios. These models, trained on a large-scale database, can predict code compliance factors with calibrated epistemic uncertainty, enabling fast and uncertainty-aware triage. The proposed framework can significantly reduce costs and emissions by avoiding unnecessary analyses and physical interventions across entire infrastructure portfolios."
  },
  {
    "title": "MARCOS: Deep Thinking by Markov Chain of Continuous Thoughts",
    "summary": "The current paradigm for reasoning in large language models (LLMs) involves\nmodels \"thinking out loud\" via a sequence of tokens, known as chain-of-thought\n(CoT). This approach, while effective, has several significant drawbacks.\nFirstly, inference requires autoregressive generation of often thousands of CoT\ntokens, which is slow and computationally expensive. Secondly, it constrains\nreasoning to the discrete space of tokens, creating an information bottleneck\nacross reasoning steps. Thirdly, it fundamentally entangles reasoning with\ntoken generation, forcing LLMs to \"think while speaking,\" which causes\npotentially short-sighted reasoning. In light of these limitations, we\nre-imagine reasoning in LLMs and present a new paradigm: MARCOS. In our\napproach, rather than autoregressively generating tokens, we model reasoning as\na hidden Markov chain of continuous, high-dimensional \"thoughts\". Each\nreasoning step involves a transition of the internal thoughts, where explicit\nreasoning steps (which may consist of hundreds of tokens) serve as observable\nvariables, which are windows to peek into the implicit thoughts. Since this\nlatent process is incompatible with the standard supervised learning, we\nfurther propose a two-phase variational training scheme. Our experiments on\nthree benchmarks demonstrate that MARCOS outperforms existing continuous\nreasoning methods and, for the first time, achieves performance comparable to\ntoken-based CoT, even surpassing it by 4.7% on GSM8K with up to 15.7x speedup\nin inference. Beyond this, MARCOS offers additional advantages, such as\nstep-level instead of token-level control over randomness, opening significant\nopportunities for reinforcement learning and reasoning in LLMs.",
    "published": "2025-09-29T16:44:22Z",
    "arxiv_url": "http://arxiv.org/abs/2509.25020v1",
    "arxiv_id": "2509.25020v1",
    "llm_summary": "This research paper proposes a new paradigm for reasoning in large language models (LLMs) called MARCOS, which differs from the traditional chain-of-thought (CoT) approach by modeling reasoning as a hidden Markov chain of continuous thoughts. MARCOS uses a two-phase variational training scheme to train LLMs, allowing for more efficient and effective reasoning. The proposed approach has been tested on three benchmarks and demonstrated significant improvements over existing methods, including a 4.7% speedup in inference and comparable performance to token-based CoT."
  },
  {
    "title": "Uncertainty-Aware Deep Learning for Wildfire Danger Forecasting",
    "summary": "Wildfires are among the most severe natural hazards, posing a significant\nthreat to both humans and natural ecosystems. The growing risk of wildfires\nincreases the demand for forecasting models that are not only accurate but also\nreliable. Deep Learning (DL) has shown promise in predicting wildfire danger;\nhowever, its adoption is hindered by concerns over the reliability of its\npredictions, some of which stem from the lack of uncertainty quantification. To\naddress this challenge, we present an uncertainty-aware DL framework that\njointly captures epistemic (model) and aleatoric (data) uncertainty to enhance\nshort-term wildfire danger forecasting. In the next-day forecasting, our\nbest-performing model improves the F1 Score by 2.3% and reduces the Expected\nCalibration Error by 2.1% compared to a deterministic baseline, enhancing both\npredictive skill and calibration. Our experiments confirm the reliability of\nthe uncertainty estimates and illustrate their practical utility for decision\nsupport, including the identification of uncertainty thresholds for rejecting\nlow-confidence predictions and the generation of well-calibrated wildfire\ndanger maps with accompanying uncertainty layers. Extending the forecast\nhorizon up to ten days, we observe that aleatoric uncertainty increases with\ntime, showing greater variability in environmental conditions, while epistemic\nuncertainty remains stable. Finally, we show that although the two uncertainty\ntypes may be redundant in low-uncertainty cases, they provide complementary\ninsights under more challenging conditions, underscoring the value of their\njoint modeling for robust wildfire danger prediction. In summary, our approach\nsignificantly improves the accuracy and reliability of wildfire danger\nforecasting, advancing the development of trustworthy wildfire DL systems.",
    "published": "2025-09-29T16:43:17Z",
    "arxiv_url": "http://arxiv.org/abs/2509.25017v1",
    "arxiv_id": "2509.25017v1",
    "llm_summary": "This research presents a new uncertainty-aware deep learning (DL) framework for predicting wildfire danger. The framework jointly captures both epistemic (model) and aleatoric (data) uncertainty, leading to improved short-term forecasting accuracy and reliability. The approach is tested and validated in a series of experiments, demonstrating its practical utility for decision-making and wildfire danger mapping."
  },
  {
    "title": "CLASP: Adaptive Spectral Clustering for Unsupervised Per-Image\n  Segmentation",
    "summary": "We introduce CLASP (Clustering via Adaptive Spectral Processing), a\nlightweight framework for unsupervised image segmentation that operates without\nany labeled data or finetuning. CLASP first extracts per patch features using a\nself supervised ViT encoder (DINO); then, it builds an affinity matrix and\napplies spectral clustering. To avoid manual tuning, we select the segment\ncount automatically with a eigengap silhouette search, and we sharpen the\nboundaries with a fully connected DenseCRF. Despite its simplicity and training\nfree nature, CLASP attains competitive mIoU and pixel accuracy on COCO Stuff\nand ADE20K, matching recent unsupervised baselines. The zero training design\nmakes CLASP a strong, easily reproducible baseline for large unannotated\ncorpora especially common in digital advertising and marketing workflows such\nas brand safety screening, creative asset curation, and social media content\nmoderation",
    "published": "2025-09-29T16:41:30Z",
    "arxiv_url": "http://arxiv.org/abs/2509.25016v1",
    "arxiv_id": "2509.25016v1",
    "llm_summary": "This research paper introduces a lightweight framework for unsupervised image segmentation called CLASP, which operates without labeled data or fine-tuning. CLASP uses a self-supervised ViT encoder to extract features, builds an affinity matrix, and applies spectral clustering to segment images. The framework achieves competitive results on two popular benchmark datasets, COCO Stuff and ADE20K, and is particularly useful for large unannotated image corpora."
  },
  {
    "title": "Score-based Membership Inference on Diffusion Models",
    "summary": "Membership inference attacks (MIAs) against diffusion models have emerged as\na pressing privacy concern, as these models may inadvertently reveal whether a\ngiven sample was part of their training set. We present a theoretical and\nempirical study of score-based MIAs, focusing on the predicted noise vectors\nthat diffusion models learn to approximate. We show that the expected denoiser\noutput points toward a kernel-weighted local mean of nearby training samples,\nsuch that its norm encodes proximity to the training set and thereby reveals\nmembership. Building on this observation, we propose SimA, a single-query\nattack that provides a principled, efficient alternative to existing\nmulti-query methods. SimA achieves consistently strong performance across\nvariants of DDPM, Latent Diffusion Model (LDM). Notably, we find that Latent\nDiffusion Models are surprisingly less vulnerable than pixel-space models, due\nto the strong information bottleneck imposed by their latent auto-encoder. We\nfurther investigate this by differing the regularization hyperparameters\n($\\beta$ in $\\beta$-VAE) in latent channel and suggest a strategy to make LDM\ntraining more robust to MIA. Our results solidify the theory of score-based\nMIAs, while highlighting that Latent Diffusion class of methods requires better\nunderstanding of inversion for VAE, and not simply inversion of the Diffusion\nprocess",
    "published": "2025-09-29T16:28:55Z",
    "arxiv_url": "http://arxiv.org/abs/2509.25003v1",
    "arxiv_id": "2509.25003v1",
    "llm_summary": "This research paper presents a theoretical and empirical study on membership inference attacks (MIAs) against diffusion models, which may inadvertently reveal whether a given sample was part of their training set. The authors propose a single-query attack called SimA, which achieves consistently strong performance across variants of diffusion models, including Latent Diffusion Models. The study also highlights the vulnerability of Latent Diffusion Models to MIAs, particularly when regularization hyperparameters are not properly tuned."
  },
  {
    "title": "LVT: Large-Scale Scene Reconstruction via Local View Transformers",
    "summary": "Large transformer models are proving to be a powerful tool for 3D vision and\nnovel view synthesis. However, the standard Transformer's well-known quadratic\ncomplexity makes it difficult to scale these methods to large scenes. To\naddress this challenge, we propose the Local View Transformer (LVT), a\nlarge-scale scene reconstruction and novel view synthesis architecture that\ncircumvents the need for the quadratic attention operation. Motivated by the\ninsight that spatially nearby views provide more useful signal about the local\nscene composition than distant views, our model processes all information in a\nlocal neighborhood around each view. To attend to tokens in nearby views, we\nleverage a novel positional encoding that conditions on the relative geometric\ntransformation between the query and nearby views. We decode the output of our\nmodel into a 3D Gaussian Splat scene representation that includes both color\nand opacity view-dependence. Taken together, the Local View Transformer enables\nreconstruction of arbitrarily large, high-resolution scenes in a single forward\npass. See our project page for results and interactive demos\nhttps://toobaimt.github.io/lvt/.",
    "published": "2025-09-29T16:24:34Z",
    "arxiv_url": "http://arxiv.org/abs/2509.25001v1",
    "arxiv_id": "2509.25001v1",
    "llm_summary": "This research paper proposes a new architecture, the Local View Transformer (LVT), which addresses the scalability issue of large transformer models in 3D vision and view synthesis. The LVT circumvents the quadratic complexity of the standard Transformer by processing information in a local neighborhood around each view and leveraging a novel positional encoding to attend to tokens in nearby views. The model can reconstruct arbitrarily large, high-resolution scenes in a single forward pass, enabling the reconstruction of scenes in a single forward pass."
  }
]




// NEWER SUMMARIES

  {
    "title": "EgoNight: Towards Egocentric Vision Understanding at Night with a\n  Challenging Benchmark",
    "summary": "Most existing benchmarks for egocentric vision understanding focus primarily\non daytime scenarios, overlooking the low-light conditions that are inevitable\nin real-world applications. To investigate this gap, we present EgoNight, the\nfirst comprehensive benchmark for nighttime egocentric vision, with visual\nquestion answering (VQA) as the core task. A key feature of EgoNight is the\nintroduction of day-night aligned videos, which enhance night annotation\nquality using the daytime data and reveal clear performance gaps between\nlighting conditions. To achieve this, we collect both synthetic videos rendered\nby Blender and real-world recordings, ensuring that scenes and actions are\nvisually and temporally aligned. Leveraging these paired videos, we construct\nEgoNight-VQA, supported by a novel day-augmented night auto-labeling engine and\nrefinement through extensive human verification. Each QA pair is double-checked\nby annotators for reliability. In total, EgoNight-VQA contains 3658 QA pairs\nacross 90 videos, spanning 12 diverse QA types, with more than 300 hours of\nhuman work. Evaluations of state-of-the-art multimodal large language models\n(MLLMs) reveal substantial performance drops when transferring from day to\nnight, underscoring the challenges of reasoning under low-light conditions.\nBeyond VQA, EgoNight also introduces two auxiliary tasks, day-night\ncorrespondence retrieval and egocentric depth estimation at night, that further\nexplore the boundaries of existing models. We believe EgoNight-VQA provides a\nstrong foundation for advancing application-driven egocentric vision research\nand for developing models that generalize across illumination domains. All the\ndata and code will be made available upon acceptance.",
    "published": "2025-10-07T17:59:47Z",
    "arxiv_url": "http://arxiv.org/abs/2510.06218v1",
    "arxiv_id": "2510.06218v1",
    "llm_summary": "This research paper presents EgoNight, a comprehensive benchmark for nighttime egocentric vision understanding. The benchmark includes 90 videos with 3658 QA pairs, collected from both synthetic and real-world recordings, and features a novel day-augmented night auto-labeling engine. The benchmark evaluates the performance of state-of-the-art multimodal large language models on VQA and explores auxiliary tasks for generalizing across illumination domains."
  },
  {
    "title": "TaTToo: Tool-Grounded Thinking PRM for Test-Time Scaling in Tabular\n  Reasoning",
    "summary": "Process Reward Models (PRMs) have recently emerged as a powerful framework\nfor enhancing the reasoning capabilities of large reasoning models (LRMs),\nparticularly in the context of test-time scaling (TTS). However, their\npotential for supervising LRMs on tabular reasoning domains remains\nunderexplored. Through detailed empirical analyses, we identify that existing\nPRMs, though widely adopted for supervising text-only reasoning steps, struggle\nwith table-specific operations such as sub-table retrieval and schema\ninteraction, leading to critical performance bottlenecks. To address this\nlimitation, we propose TaTToo, a novel table-grounded PRM framework that (i)\nreasons explicitly over tabular reasoning steps and (ii) integrates tool-based\nverification to provide precise reward supervision. Concretely, we first design\na scalable data curation pipeline that constructs over 60k high-quality\nstep-level annotations by integrating table verification rationales with\ntool-based executions. Building on the collected data, we train TaTToo with a\ndual-stage paradigm: cold-start supervised fine-tuning to capture tool-use\nreasoning patterns, followed by reinforcement learning with tool-grounded\nreward shaping to align our model with table-based verification. We provide a\ncomprehensive evaluation of the policy improvement induced by our newly\ndesigned PRM. Across 5 challenging tabular reasoning benchmarks covering\nnumerical reasoning, fact-checking, and data analysis, TaTToo improves\ndownstream policy LRMs by 30.9% at inference, surpasses strong PRM baselines\nsuch as Qwen-2.5-Math-PRM-72B with only 8B parameters, and demonstrates strong\ngeneralizability across diverse TTS strategies.",
    "published": "2025-10-07T17:59:41Z",
    "arxiv_url": "http://arxiv.org/abs/2510.06217v1",
    "arxiv_id": "2510.06217v1",
    "llm_summary": "Researchers have developed a new table-grounded process reward model (PRM) framework, TaTToo, which addresses the limitations of existing PRMs in supervising large reasoning models on tabular reasoning domains. TaTToo explicitly reasons over tabular reasoning steps, integrates tool-based verification, and provides precise reward supervision. The proposed framework has shown significant improvements in downstream policy LRMs, surpassing strong PRM baselines and demonstrating generalizability across diverse TTS strategies."
  },
  {
    "title": "Stratified GRPO: Handling Structural Heterogeneity in Reinforcement\n  Learning of LLM Search Agents",
    "summary": "Large language model (LLM) agents increasingly rely on external tools such as\nsearch engines to solve complex, multi-step problems, and reinforcement\nlearning (RL) has become a key paradigm for training them. However, the\ntrajectories of search agents are structurally heterogeneous, where variations\nin the number, placement, and outcomes of search calls lead to fundamentally\ndifferent answer directions and reward distributions. Standard policy gradient\nmethods, which use a single global baseline, suffer from what we identify and\nformalize as cross-stratum bias-an \"apples-to-oranges\" comparison of\nheterogeneous trajectories. This cross-stratum bias distorts credit assignment\nand hinders exploration of complex, multi-step search strategies. To address\nthis, we propose Stratified GRPO, whose central component, Stratified Advantage\nNormalization (SAN), partitions trajectories into homogeneous strata based on\ntheir structural properties and computes advantages locally within each\nstratum. This ensures that trajectories are evaluated only against their true\npeers. Our analysis proves that SAN eliminates cross-stratum bias, yields\nconditionally unbiased unit-variance estimates inside each stratum, and retains\nthe global unbiasedness and unit-variance properties enjoyed by standard\nnormalization, resulting in a more pure and scale-stable learning signal. To\nimprove practical stability under finite-sample regimes, we further linearly\nblend SAN with the global estimator. Extensive experiments on diverse\nsingle-hop and multi-hop question-answering benchmarks demonstrate that\nStratified GRPO consistently and substantially outperforms GRPO by up to 11.3\npoints, achieving higher training rewards, greater training stability, and more\neffective search policies. These results establish stratification as a\nprincipled remedy for structural heterogeneity in RL for LLM search agents.",
    "published": "2025-10-07T17:59:13Z",
    "arxiv_url": "http://arxiv.org/abs/2510.06214v1",
    "arxiv_id": "2510.06214v1",
    "llm_summary": "This research proposes a new method for training large language model (LLM) agents called Stratified GRPO, which addresses the problem of cross-stratum bias in reinforcement learning (RL) by partitioning trajectories into homogeneous strata based on their structural properties. The proposed method, Stratified Advantage Normalization (SAN), ensures that the agent's training rewards are unbiased and scale-stable, and it achieves higher training rewards and greater training stability compared to the standard GRPO method. The results demonstrate that Stratified GRPO outperforms GRPO by up to 11.3 points in single-hop and"
  },
  {
    "title": "Reference Grounded Skill Discovery",
    "summary": "Scaling unsupervised skill discovery algorithms to high-DoF agents remains\nchallenging. As dimensionality increases, the exploration space grows\nexponentially, while the manifold of meaningful skills remains limited.\nTherefore, semantic meaningfulness becomes essential to effectively guide\nexploration in high-dimensional spaces. In this work, we present\nReference-Grounded Skill Discovery (RGSD), a novel algorithm that grounds skill\ndiscovery in a semantically meaningful latent space using reference data. RGSD\nfirst performs contrastive pretraining to embed motions on a unit hypersphere,\nclustering each reference trajectory into a distinct direction. This grounding\nenables skill discovery to simultaneously involve both imitation of reference\nbehaviors and the discovery of semantically related diverse behaviors. On a\nsimulated SMPL humanoid with 359-D observations and 69-D actions, RGSD learns\nstructured skills including walking, running, punching, and side stepping, and\nalso discovers related novel behaviors. In downstream control tasks, RGSD\noutperforms imitation-based skill acquisition baselines. Our results suggest\nthat lightweight reference-guided grounding offers a practical path to\ndiscovering semantically rich and structured skills in high-DoF systems.",
    "published": "2025-10-07T17:55:01Z",
    "arxiv_url": "http://arxiv.org/abs/2510.06203v1",
    "arxiv_id": "2510.06203v1",
    "llm_summary": "This research paper proposes a novel algorithm called Reference-Grounded Skill Discovery (RGSD) that grounds skill discovery in a semantically meaningful latent space using reference data. By performing contrastive pretraining on a unit hypersphere, RGSD enables skill discovery to simultaneously involve both imitation of reference behaviors and the discovery of semantically related diverse behaviors. The algorithm outperforms imitation-based skill acquisition baselines in downstream control tasks, suggesting a practical path to discovering semantically rich and structured skills in high-DoF systems."
  },
  {
    "title": "TokenChain: A Discrete Speech Chain via Semantic Token Modeling",
    "summary": "Machine Speech Chain, simulating the human perception-production loop, proves\neffective in jointly improving ASR and TTS. We propose TokenChain, a fully\ndiscrete speech chain coupling semantic-token ASR with a two-stage TTS: an\nautoregressive text-to-semantic model co-trained with ASR and a\nmasked-generative semantic-to-acoustic model for synthesis only. End-to-end\nfeedback across the text interface is enabled with straight-through\nargmax/Gumbel-Softmax and balanced with supervised ASR via dynamic weight\naveraging. Ablations examine optimal temperature schedules for in- and\ncross-domain transfer. Evaluation reveals TokenChain surpasses baseline\naccuracy 2-6 epochs earlier and yields 5-13% lower equal-epoch error with\nstable T2S on LibriSpeech, and reduces relative ASR WER by 56% and T2S WER by\n31% on TED-LIUM with minimal forgetting, showing that chain learning remains\neffective with token interfaces and models.",
    "published": "2025-10-07T17:54:12Z",
    "arxiv_url": "http://arxiv.org/abs/2510.06201v1",
    "arxiv_id": "2510.06201v1",
    "llm_summary": "This research paper proposes a novel approach to improving Automatic Speech Recognition (ASR) and Text-to-Speech (TTS) by simulating the human perception-production loop. TokenChain, a fully discrete speech chain coupling ASR with a two-stage TTS model, achieves state-of-the-art performance on LibriSpeech and TED-LIUM datasets, outperforming baseline models by 2-6 epochs and 56% and 31% respectively. The proposed approach enables end-to-end feedback across the text interface, facilitating stable and efficient transfer learning."
  },
  {
    "title": "StarEmbed: Benchmarking Time Series Foundation Models on Astronomical\n  Observations of Variable Stars",
    "summary": "Time series foundation models (TSFMs) are increasingly being adopted as\nhighly-capable general-purpose time series representation learners. Although\ntheir training corpora are vast, they exclude astronomical time series data.\nObservations of stars produce peta-scale time series with unique challenges\nincluding irregular sampling and heteroskedasticity. We introduce StarEmbed,\nthe first public benchmark for rigorous and standardized evaluation of\nstate-of-the-art TSFMs on stellar time series observations (``light curves'').\nWe benchmark on three scientifically-motivated downstream tasks: unsupervised\nclustering, supervised classification, and out-of-distribution source\ndetection. StarEmbed integrates a catalog of expert-vetted labels with\nmulti-variate light curves from the Zwicky Transient Facility, yielding ~40k\nhand-labeled light curves spread across seven astrophysical classes. We\nevaluate the zero-shot representation capabilities of three TSFMs (MOIRAI,\nChronos, Chronos-Bolt) and a domain-specific transformer (Astromer) against\nhandcrafted feature extraction, the long-standing baseline in the astrophysics\nliterature. Our results demonstrate that these TSFMs, especially the Chronos\nmodels, which are trained on data completely unlike the astronomical\nobservations, can outperform established astrophysics-specific baselines in\nsome tasks and effectively generalize to entirely new data. In particular,\nTSFMs deliver state-of-the-art performance on our out-of-distribution source\ndetection benchmark. With the first benchmark of TSFMs on astronomical time\nseries data, we test the limits of their generalization and motivate a paradigm\nshift in time-domain astronomy from using task-specific, fully supervised\npipelines toward adopting generic foundation model representations for the\nanalysis of peta-scale datasets from forthcoming observatories.",
    "published": "2025-10-07T17:53:56Z",
    "arxiv_url": "http://arxiv.org/abs/2510.06200v1",
    "arxiv_id": "2510.06200v1",
    "llm_summary": "This paper introduces StarEmbed, a benchmark for evaluating time series foundation models on stellar time series observations. The authors compare the performance of three state-of-the-art TSFMs (MOIRAI, Chronos, and Astromer) against handcrafted feature extraction and established astrophysics-specific baselines. The results show that TSFMs can outperform these baselines in some tasks, particularly in out-of-distribution source detection. The authors propose a paradigm shift in time-domain astronomy by adopting generic foundation model representations for peta-scale datasets."
  },
  {
    "title": "Latent Speech-Text Transformer",
    "summary": "Auto-regressive speech-text models are typically pre-trained on a large\nnumber of interleaved sequences of text tokens and raw speech encoded as speech\ntokens using vector quantization. These models have demonstrated\nstate-of-the-art performance in speech-to-speech understanding and generation\nbenchmarks, together with promising scaling laws, primarily enabled by the\nrepresentational alignment between text and speech. Nevertheless, they suffer\nfrom shortcomings, partly owing to the disproportionately longer sequences of\nspeech tokens in contrast to textual tokens. This results in a large compute\nimbalance between modalities during pre-training as well as during inference,\nand a potential hindrance to effectively aligning speech and text, ultimately\ntranslating to several orders of magnitude slower scaling laws. We introduce\nthe Latent Speech-Text Transformer (LST), which makes pre-training speech-text\nmodels more data-efficient by dynamically and inexpensively aggregating speech\ntokens into latent speech patches. These patches serve as higher-level units\nthat can either align with corresponding textual units to aid capability\ntransfer or even encapsulate common speech sequences like silences to be more\ncompute-efficient. We show that LST outperforms vanilla approaches on\nspeech-to-speech as well as text-to-text benchmarks in both data- and\ncompute-controlled settings, the former indicating more effective\nrepresentational alignment and the latter indicating steeper scaling laws for\nspeech-text models. On HellaSwag story completion, LST achieves 6.5% absolute\ngain in speech accuracy under compute-controlled training and 5.3% under\ndata-controlled training, while also improving text performance. We will\nrelease our models, code, and the evaluation data to facilitate further\nresearch.",
    "published": "2025-10-07T17:52:08Z",
    "arxiv_url": "http://arxiv.org/abs/2510.06195v1",
    "arxiv_id": "2510.06195v1",
    "llm_summary": "This research introduces the Latent Speech-Text Transformer (LST), a pre-training method that dynamically aggregates speech tokens into latent speech patches to improve data efficiency and representational alignment between text and speech. The LST outperforms vanilla approaches on both speech-to-speech and text-to-text benchmarks, demonstrating steeper scaling laws for speech-text models. The proposed method can be used to improve speech-to-speech understanding and generation, as well as text-to-text translation, by reducing the compute-imbalance between modalities."
  },
  {
    "title": "Barbarians at the Gate: How AI is Upending Systems Research",
    "summary": "Artificial Intelligence (AI) is starting to transform the research process as\nwe know it by automating the discovery of new solutions. Given a task, the\ntypical AI-driven approach is (i) to generate a set of diverse solutions, and\nthen (ii) to verify these solutions and select one that solves the problem.\nCrucially, this approach assumes the existence of a reliable verifier, i.e.,\none that can accurately determine whether a solution solves the given problem.\nWe argue that systems research, long focused on designing and evaluating new\nperformance-oriented algorithms, is particularly well-suited for AI-driven\nsolution discovery. This is because system performance problems naturally admit\nreliable verifiers: solutions are typically implemented in real systems or\nsimulators, and verification reduces to running these software artifacts\nagainst predefined workloads and measuring performance. We term this approach\nas AI-Driven Research for Systems (ADRS), which iteratively generates,\nevaluates, and refines solutions. Using penEvolve, an existing open-source ADRS\ninstance, we present case studies across diverse domains, including load\nbalancing for multi-region cloud scheduling, Mixture-of-Experts inference,\nLLM-based SQL queries, and transaction scheduling. In multiple instances, ADRS\ndiscovers algorithms that outperform state-of-the-art human designs (e.g.,\nachieving up to 5.0x runtime improvements or 50% cost reductions). We distill\nbest practices for guiding algorithm evolution, from prompt design to evaluator\nconstruction, for existing frameworks. We then discuss the broader implications\nfor the systems community: as AI assumes a central role in algorithm design, we\nargue that human researchers will increasingly focus on problem formulation and\nstrategic guidance. Our results highlight both the disruptive potential and the\nurgent need to adapt systems research practices in the age of AI.",
    "published": "2025-10-07T17:49:24Z",
    "arxiv_url": "http://arxiv.org/abs/2510.06189v1",
    "arxiv_id": "2510.06189v1",
    "llm_summary": "This research paper explores the potential of Artificial Intelligence (AI) in transforming the research process by automating the discovery of new solutions. The authors argue that systems research, which focuses on designing and evaluating algorithms, is particularly well-suited for AI-driven solution discovery. They present case studies across diverse domains and demonstrate that AI-Driven Research for Systems (ADRS) can discover algorithms that outperform state-of-the-art human designs. The authors also discuss the broader implications for the systems community, highlighting both the potential benefits and the need for adaptation in the age of AI."
  },
